import logging
from app.infrastructure.logging import request_id_var
from app.workers.document_worker import process_document_task

# Initialize logger for tracking task dispatch
logger = logging.getLogger(__name__)

def queue_processing(document_id: str):
    """
    Dispatches the document analysis task to the Celery queue.
    
    This function acts as a 'Fire and Forget' trigger. It returns 
    immediately to the API user while the AI works in the background.
    """
    
    # 1. Retrieve the Unique Request ID
    # This ID was generated by the middleware when the user first uploaded the file.
    current_rid = request_id_var.get()
    
    # 2. Log the handoff
    # This log entry is the last thing we see in the 'backend' container 
    # before the 'worker' container takes over.
    logger.info(f"Dispatching task for document {document_id}. TraceID: {current_rid}")
    
    # 3. Trigger the Celery Task
    # .delay() is the standard way to send a message to Redis/RabbitMQ.
    # We pass the request_id so the worker can set its own context for logging.
    task = process_document_task.delay(document_id, request_id=current_rid)
    
    return {
        "task_id": task.id,
        "document_id": document_id,
        "trace_id": current_rid
    }